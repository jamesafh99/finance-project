import logging
import re
import pandas as pd
from typing import Dict, cast
from src.config import DATA_RAW_DIR, DATA_PROCESSED_DIR

# Logger setup
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

# Market correction map
MARKET_CORRECTION_MAP = {
    ".L":  {"GBP": "GBp"},  # London: Labeled GBP -> Actually GBp
    ".IL": {"ILS": "ILA"},  # Israel: Labeled ILS -> Actually ILA (Agorot)
    ".J":  {"ZAR": "ZAC"},  # Johannesburg: Labeled ZAR -> Actually ZAC (Cents)
}

# Minor unit handling
MINOR_CURRENCY_MAP = {
    # United Kingdom / Ireland
    "GBp": ("GBP", 100),   # British Pence
    "GBX": ("GBP", 100),   # Alternative British Pence code
    "IGp": ("GBP", 100),   # Irish Pence (often pegged/listed as GBP base)
    
    # Europe / Africa
    "ZAC": ("ZAR", 100),   # South African Cents
    
    # Middle East
    "ILA": ("ILS", 100),   # Israeli Agorot
    "KWf": ("KWD", 1000),  # Kuwaiti Fils (Note: Divisor is 1000)
    
    # Asia
    "CNX": ("CNY", 100),   # Chinese Renminbi Cents
    "HKc": ("HKD", 100),   # Hong Kong Cents
    "SGc": ("SGD", 100),   # Singapore Cents
    "TWc": ("TWD", 100),   # Taiwan Cents
    "MYc": ("MYR", 100)    # Malaysian Sen
}

# Function for advanced ticker cleaning
def cleaning_ticker(ticker: str) -> str:
    # Step 1: Handle Suffixes (remove =X, =F and everything after)
    clean_ticker = ticker.split('=')[0]
    
    # Step 2: Handle Prefixes (remove ^ only from start)
    clean_ticker = clean_ticker.lstrip('^')
    
    # Step 3: Sanitize (remove residual characters like '-' in BTC-USD)
    clean_ticker = re.sub(r'[^a-zA-Z0-9]', '', clean_ticker)

    return clean_ticker

# Function to load 'data_manifest.csv'
def load_manifest() -> pd.DataFrame:
    """Loads the manifest file generated by the downloader"""
    manifest_path = DATA_RAW_DIR / "data_manifest.csv"

    if not manifest_path.exists():
        raise FileNotFoundError(f"Manifest not found at {manifest_path}. Run 00_notebook first.")
    
    return pd.read_csv(manifest_path)

# Function to load raw data
def load_raw_data(ticker: str) -> pd.Series:
    """Loads a single raw CSV, sets index to Date, and returns the Close series"""

    clean_ticker = cleaning_ticker(ticker)
    filepath = DATA_RAW_DIR / "prices" / f"{clean_ticker}_prices.csv"

    if not filepath.exists():
        logger.error(f"File not found for {clean_ticker}")
        return pd.Series(dtype=float)

    df = pd.read_csv(filepath, parse_dates=["Date"], index_col="Date")

    # Checking for duplication
    if df.index.duplicated().any():
        logger.warning(f"Duplicate index found in {clean_ticker}. Keeping last.")
        df = df[~df.index.duplicated(keep="last")]
    
    # Return 'Close' prices
    return df["Close"].rename(clean_ticker)

# Function to convert currencies to Target (e.g. GBP)
def convert_currency(target_currency: str) -> pd.DataFrame:
    """
    Main Logic:
    1. Reads Manifest.
    2. Loads all assets.
    3. Normalizes currencies to Target (e.g. GBP) using downloaded FX pairs.
    4. Aligns dates and fills NaNs.
    """

    logger.info("Starting data processing and normalization engine...")
    manifest = load_manifest()

    # 1. Separate Assets vs Currencies
    fx_map: Dict[str, pd.Series] = {}

    # Loading FX pairs first to perform conversions
    fx_rows = manifest[manifest["type"] == "CURRENCY"]

    for _, row in fx_rows.iterrows():
        ticker = row["ticker"]
        series = load_raw_data(ticker)
        if not series.empty:
            fx_map[ticker] = series

    # 2. Assets processing
    asset_rows = manifest[manifest["source"] != "FX_dependency"]
    processed_data = {}

    for _, row in asset_rows.iterrows():
        ticker = row["ticker"]
        original_curr = row["original_currency"]
        asset_type = row["type"]

        # Load prices
        series = load_raw_data(ticker)
        if series.empty:
            continue

        ticker_name = str(series.name)

        for suffix, corrections in MARKET_CORRECTION_MAP.items():
            if ticker.endswith(suffix):
                if original_curr in corrections:
                    new_curr = corrections[original_curr]
                    logger.info(f"Market Correction ({suffix}): Renaming currency {original_curr} -> {new_curr}")
                    original_curr = new_curr
                break # Stop checking suffixes once match is found

        # Minor currency normalization
        if original_curr in MINOR_CURRENCY_MAP:
            major_curr, divisor = MINOR_CURRENCY_MAP[original_curr]

            # Applying normalization
            series = (series / divisor).rename(ticker_name)
            logger.info(f"Normalized {ticker}: {original_curr} -> {major_curr} (Divisor: {divisor})")

            # Updating currency variable for the next step (FX conversion)
            original_curr = major_curr

        # Currency normalization logic
        if asset_type == "RATE":
            # Risk-free rate (like ^IRX) are usually percentages
            if series.abs().max() > 1:
                series = (series / 100).rename(ticker_name)
                logger.info(f"Converted {ticker} from percentage to decimal format.")

        elif original_curr != target_currency:
            # Finding the FX pair to convert currency
            required_pair = f"{target_currency}{original_curr}=X"

            if required_pair in fx_map:
                fx_rate = fx_map[required_pair]

                # Aligning dates
                aligned_df = pd.concat([series, fx_rate], axis=1, join="inner")
                aligned_df.columns = ["Price", "FX"]

                # Conversion
                series = (aligned_df["Price"] / aligned_df["FX"]).rename(ticker_name)
                logger.info(f"Converted {ticker} from {original_curr} to {target_currency}")
            else:
                logger.error(f"Missing FX pair {required_pair} for {ticker}. Skipping conversion (RISK!)")
        
        # Processed data
        processed_data[ticker_name] = series

    # 3. Consolidation
    logger.info("Merging all assets into a single universe...")
    universe = cast(pd.DataFrame, pd.concat(processed_data.values(), axis=1, keys=processed_data.keys()))

    # 4. Cleaning (Handling missing data) and Forward Fill to handle holidays from different countries
    universe = universe.ffill()
    universe = universe.dropna()
    universe = universe.sort_index(axis=1)
    logger.info(f"Final universe shape: (rows:{universe.shape[0]}, cols: {universe.shape[1]})")

    # 6. Saving 'asset_universe.csv'
    filepath = DATA_PROCESSED_DIR / "asset_universe.csv"
    DATA_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
    universe.to_csv(filepath, index=True)
    logger.info(f"Saved processed data to {filepath}")

    return universe

# Testing
if __name__ == "__main__":
    print(convert_currency("GBP"))